{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cweMlOB0L4mG"
      },
      "source": [
        "# Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po-TEvrWMJ_a"
      },
      "source": [
        "## CUDA Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-lgwhE1N5_7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfe196e2-23d9-493a-8e58-7263926e4af6"
      },
      "source": [
        "%%writefile cuda_stuff.cuh\n",
        "#ifndef cuda_stuff_H\n",
        "#define cuda_stuff_H\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "//MACRO TO DEBUG CUDA FUNCTIONS\n",
        "/** Error checking,\n",
        " *  taken from https://stackoverflow.com/questions/14038589/what-is-the-canonical-way-to-check-for-errors-using-the-cuda-runtime-api\n",
        " */\n",
        "#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }\n",
        "inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)\n",
        "{\n",
        "   if (code != cudaSuccess)\n",
        "   {\n",
        "      fprintf(stderr,\"GPUassert: %s %s %d\\n\", cudaGetErrorString(code), file, line);\n",
        "      if (abort) exit(code);\n",
        "   }\n",
        "}\n",
        "\n",
        "void device_synchronize();\n",
        "\n",
        "#endif\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing cuda_stuff.cuh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iivrxLaYOYPh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b28f8d5f-5ff1-400e-8b8e-9081b6154695"
      },
      "source": [
        "%%writefile cuda_stuff.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#include \"cuda_stuff.cuh\"\n",
        "\n",
        "void device_synchronize(){\n",
        "    gpuErrchk(cudaDeviceSynchronize());\n",
        "}"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing cuda_stuff.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fsEMpauK8lW"
      },
      "source": [
        "## Matrix Tools"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A97U902HMog4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9f5dcc8-0a7b-400a-aec7-0bced086a82e"
      },
      "source": [
        "%%writefile fmatrix.cuh\n",
        "#ifndef fmatrices_H\n",
        "#define fmatrices_H\n",
        "#include <stddef.h>\n",
        "\n",
        "typedef struct {\n",
        "    float* data;\n",
        "    size_t cols;\n",
        "    size_t rows;\n",
        "} fmatrix;\n",
        "\n",
        "/* transform matrix index to vector offset\n",
        "   Since CUDA uses column major,\n",
        "   nb_rows = number of rows */\n",
        "#define IDX2C(i,j,nb_rows) (((j)*(nb_rows))+(i))\n",
        "\n",
        "/* Access element (i,j) of matrix mat */\n",
        "#define getfm(mat,i,j) (mat.data[IDX2C(i,j,mat.rows)])\n",
        "\n",
        "\n",
        "size_t fmatrix_elements(fmatrix mat);\n",
        "size_t fmatrix_size(fmatrix mat);\n",
        "void fmatrix_init(fmatrix mat, float f);\n",
        "/** Assert that the matrix is coherent: all fields nonzero. */\n",
        "void fmatrix_assert();\n",
        "\n",
        "fmatrix fmatrix_create_on_host(size_t rows, size_t cols);\n",
        "fmatrix fmatrix_create_on_device(size_t rows, size_t cols);\n",
        "\n",
        "void fmatrix_data_to_host(fmatrix mat_host, fmatrix mat_device);\n",
        "void fmatrix_data_to_device(fmatrix mat_host, fmatrix mat_device);\n",
        "\n",
        "void fmatrix_free_on_host(fmatrix* mat);\n",
        "void fmatrix_free_on_device(fmatrix* mat);\n",
        "\n",
        "/** Print the first nb rows of the matrix mat\n",
        " *  on the host.\n",
        " *  If nb<0, print all rows.\n",
        " */\n",
        "void fmatrix_host_print(fmatrix mat, int nb=-1);\n",
        "\n",
        "/** Print the first nb rows of the matrix mat\n",
        " *  on the device.\n",
        " *  If nb<0, print all rows.\n",
        " */\n",
        "void fmatrix_device_print(fmatrix mat, int nb=-1);\n",
        "\n",
        "#endif\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing fmatrix.cuh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGwZ36ifWQ-d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0577c9b-c917-4940-ee29-911e378a622e"
      },
      "source": [
        "%%writefile fmatrix.cu\n",
        "#include <assert.h>\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#include \"cuda_stuff.cuh\"\n",
        "#include \"fmatrix.cuh\"\n",
        "\n",
        "size_t fmatrix_elements(fmatrix mat) {\n",
        "     return mat.cols*mat.rows;\n",
        "}\n",
        "\n",
        "size_t fmatrix_size(fmatrix mat) {\n",
        "     return fmatrix_elements(mat) * sizeof(float);\n",
        "}\n",
        "\n",
        "void fmatrix_init(fmatrix mat, float f) {\n",
        "    for (int i = 0; i < mat.rows; i++){\n",
        "        for (int j = 0; j < mat.cols; j++){\n",
        "            mat.data[IDX2C(i,j,mat.rows)] = f;\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "void fmatrix_assert(fmatrix mat) {\n",
        "    assert(mat.data);\n",
        "    assert(mat.cols);\n",
        "    assert(mat.rows);\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "fmatrix fmatrix_create_on_host(size_t rows, size_t cols) {\n",
        "    assert(cols>0);\n",
        "    assert(rows>0);\n",
        "    fmatrix mat;\n",
        "    mat.cols = cols;\n",
        "    mat.rows = rows;\n",
        "    mat.data = (float*)malloc(fmatrix_size(mat));\n",
        "    assert(mat.data);\n",
        "    return mat;\n",
        "}\n",
        "\n",
        "fmatrix fmatrix_create_on_device(size_t rows, size_t cols) {\n",
        "    assert(cols>0);\n",
        "    assert(rows>0);\n",
        "    fmatrix mat;\n",
        "    mat.cols = cols;\n",
        "    mat.rows = rows;\n",
        "    gpuErrchk(\n",
        "        cudaMalloc((void **)&(mat.data), fmatrix_size(mat))\n",
        "    );\n",
        "    return mat;\n",
        "}\n",
        "\n",
        "void fmatrix_data_to_device(fmatrix mat_host, fmatrix mat_device) {\n",
        "    fmatrix_assert(mat_host);\n",
        "    fmatrix_assert(mat_device);\n",
        "    assert(mat_host.cols==mat_device.cols);\n",
        "    assert(mat_host.rows==mat_device.rows);\n",
        "    gpuErrchk(\n",
        "        cudaMemcpy( mat_device.data, mat_host.data,\n",
        "                   fmatrix_size(mat_host),\n",
        "                   cudaMemcpyHostToDevice\n",
        "                   )\n",
        "        );\n",
        "}\n",
        "\n",
        "void fmatrix_data_to_host(fmatrix mat_host, fmatrix mat_device) {\n",
        "    fmatrix_assert(mat_host);\n",
        "    fmatrix_assert(mat_device);\n",
        "    assert(mat_host.cols==mat_device.cols);\n",
        "    assert(mat_host.rows==mat_device.rows);\n",
        "    gpuErrchk(\n",
        "        cudaMemcpy( mat_host.data, mat_device.data,\n",
        "                   fmatrix_size(mat_device),\n",
        "                   cudaMemcpyDeviceToHost\n",
        "                   )\n",
        "        );\n",
        "}\n",
        "\n",
        "void fmatrix_free_on_host(fmatrix* mat) {\n",
        "    fmatrix_assert(*mat);\n",
        "  free(mat->data);\n",
        "  mat->data = 0;\n",
        "  mat->cols = 0;\n",
        "  mat->rows = 0;\n",
        "}\n",
        "\n",
        "void fmatrix_free_on_device(fmatrix* mat) {\n",
        "    fmatrix_assert(*mat);\n",
        "  gpuErrchk(cudaFree(mat->data));\n",
        "  mat->data = 0;\n",
        "  mat->cols = 0;\n",
        "  mat->rows = 0;\n",
        "}\n",
        "\n",
        "void fmatrix_host_print(fmatrix mat, int nb){\n",
        "    if (nb<0 || nb > mat.rows) {\n",
        "        nb = mat.rows;\n",
        "    }\n",
        "    printf(\"[\\n\");\n",
        "    for (int i = 0 ; i < nb; i++){\n",
        "      for (int j = 0 ; j<mat.cols; j++){\n",
        "        printf(\"%f\", getfm(mat,i,j));\n",
        "        if (j+1<mat.cols) {\n",
        "          printf(\",\\t\");\n",
        "        }\n",
        "      }\n",
        "      if (i+1<nb) {\n",
        "        printf(\";\\n\");\n",
        "      }\n",
        "    }\n",
        "    if (nb < mat.rows) {\n",
        "      printf(\"\\n...\\n\");\n",
        "    }\n",
        "  printf(\"\\n]\\n\");\n",
        "}\n",
        "\n",
        "void fmatrix_device_print(fmatrix mat, int nb){\n",
        "   // allocate copy\n",
        "   fmatrix tmp = fmatrix_create_on_host(mat.rows, mat.cols);\n",
        "   fmatrix_data_to_host(tmp, mat);\n",
        "   fmatrix_host_print(tmp,nb);\n",
        "   fmatrix_free_on_host(&tmp);\n",
        "}\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing fmatrix.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tM266RRGjwUH"
      },
      "source": [
        "## Matrix Math"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNjf6dkCfh9t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d982f712-b7b1-4002-a0e9-86ef7b7ff347"
      },
      "source": [
        "%%writefile sgemm.cuh\n",
        "#ifndef sgemm_H\n",
        "#define sgemm_H\n",
        "\n",
        "#include <string>\n",
        "#include \"fmatrix.cuh\"\n",
        "\n",
        "void mat_mul(fmatrix A, fmatrix B, fmatrix C, std::string arg);\n",
        "\n",
        "#endif"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing sgemm.cuh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdwAnQevYMQX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f6c6e2d-a437-4cf9-f291-6d8ce0335c5f"
      },
      "source": [
        "%%writefile sgemm.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <string>\n",
        "#include <time.h>\n",
        "#include <math.h>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include \"cublas_v2.h\"\n",
        "\n",
        "#include \"cuda_stuff.cuh\"\n",
        "#include \"sgemm.cuh\"\n",
        "#include \"fmatrix.cuh\"\n",
        "\n",
        "#define THREADS_PER_BLOCK 1024\n",
        "#define TILE_WIDTH 32\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "static cublasHandle_t handle;\n",
        "static int cublas_init = 0;\n",
        "\n",
        "/* basic matrix multiplication C = alpha*A*B + beta*C on host as reference for the speedup */\n",
        "void matrixMultiplication_basic_host(float alpha, fmatrix A, fmatrix B, float beta, fmatrix C) {\n",
        "  float tmp = 0;\n",
        "  for (int i = 0; i<A.rows; i++){\n",
        "    for (int j = 0; j<B.cols; j++){\n",
        "      for (int k = 0; k<A.cols; k++){\n",
        "        tmp += alpha * getfm(A,i, k) * getfm(B, k, j);\n",
        "      }\n",
        "      getfm(C, i, j) = beta * getfm(C, i, j) + tmp;\n",
        "      tmp = 0;\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "/* TODO : 3 different versions of matrix multiplication C = alpha*A*B + beta*C on device */\n",
        "__global__\n",
        "void matmul_basic_kernel(float alpha, float *A, float *B, float beta, float *C, int nb_ColA, int nb_ColB, int nb_LigneA, int nb_LigneB) {\n",
        "  int row = threadIdx.y + blockIdx.y * blockDim.y;\n",
        "  int col = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "  if (row < nb_LigneA && col < nb_ColB){\n",
        "    float tmp = 0.0f;\n",
        "    for (int k = 0; k < nb_LigneB; k++){\n",
        "      tmp += alpha * A[IDX2C(row, k, nb_LigneA)] * B[IDX2C(col, k, nb_LigneB)]; /* Calcul et stockage du résultat du produit entre A et B */\n",
        "    }\n",
        "    C[IDX2C(row, col, nb_LigneA)] = beta * C[IDX2C(row, col, nb_LigneA)] + tmp; /* Calcul final en ajoutant la valeur de C et mise à jour de la matrice résultat */\n",
        "  }\n",
        "}\n",
        "\n",
        "void matrixMultiplication_basic(float alpha, fmatrix d_A, fmatrix d_B, float beta, fmatrix d_C) {\n",
        "  dim3 dimBlock(TILE_WIDTH, TILE_WIDTH);\n",
        "  dim3 dimGrid((d_A.rows + TILE_WIDTH - 1)/TILE_WIDTH, (d_B.cols + TILE_WIDTH - 1)/TILE_WIDTH);\n",
        "\n",
        "  matmul_basic_kernel <<< dimGrid, dimBlock >>> (alpha, d_A.data, d_B.data, beta, d_C.data, d_A.cols, d_B.cols, d_A.rows, d_B.rows);\n",
        "\n",
        "}\n",
        "\n",
        "/**********************/\n",
        "__global__\n",
        "void matmul_tiled_kernel(float alpha, float *A, float *B, float beta, float *C, int nb_ColA, int nb_ColB, int nb_LigneA, int nb_LigneB){\n",
        "  __shared__ float tileA[TILE_WIDTH][TILE_WIDTH];\n",
        "  __shared__ float tileB[TILE_WIDTH][TILE_WIDTH];\n",
        "\n",
        "  int row = blockIdx.y * TILE_WIDTH + threadIdx.y;\n",
        "  int col = blockIdx.x * TILE_WIDTH + threadIdx.x;\n",
        "\n",
        "  float sum = 0.0f;\n",
        "\n",
        "  for (int t = 0; t < (nb_ColA + TILE_WIDTH - 1) / TILE_WIDTH; ++t) {\n",
        "      if (row < nb_LigneA && t * TILE_WIDTH + threadIdx.x < nb_ColA)\n",
        "          tileA[threadIdx.y][threadIdx.x] = A[row * nb_ColA + t * TILE_WIDTH + threadIdx.x];\n",
        "      else\n",
        "          tileA[threadIdx.y][threadIdx.x] = 0.0f;\n",
        "\n",
        "      if (col < nb_ColB && t * TILE_WIDTH + threadIdx.y < nb_ColA)\n",
        "          tileB[threadIdx.y][threadIdx.x] = B[(t * TILE_WIDTH + threadIdx.y) * nb_ColB + col];\n",
        "      else\n",
        "          tileB[threadIdx.y][threadIdx.x] = 0.0f;\n",
        "\n",
        "      __syncthreads();\n",
        "\n",
        "      for (int k = 0; k < TILE_WIDTH; ++k) {\n",
        "          sum += tileA[threadIdx.y][k] * tileB[k][threadIdx.x];\n",
        "      }\n",
        "\n",
        "      __syncthreads();\n",
        "  }\n",
        "\n",
        "  if (row < nb_LigneA && col < nb_ColB)\n",
        "      C[row * nb_ColB + col] = alpha * sum + beta * C[row * nb_ColB + col];\n",
        "}\n",
        "\n",
        "\n",
        "void matrixMultiplication_tiled(float alpha, fmatrix d_A, fmatrix d_B, float beta, fmatrix d_C){\n",
        "  dim3 dimBlock(TILE_WIDTH, TILE_WIDTH);\n",
        "  dim3 dimGrid((d_A.rows + TILE_WIDTH - 1) / TILE_WIDTH, (d_B.cols + TILE_WIDTH - 1) / TILE_WIDTH);\n",
        "\n",
        "  matmul_tiled_kernel <<< dimGrid, dimBlock >>> (alpha, d_A.data, d_B.data, beta, d_C.data, d_A.cols, d_B.cols, d_A.rows, d_B.rows);\n",
        "}\n",
        "\n",
        "/**********************/\n",
        "void matrixMultiplication_cublas(float alpha, fmatrix d_A, fmatrix d_B, float beta, fmatrix d_C){\n",
        "  cublasHandle_t handle;\n",
        "  cublasCreate(&handle);\n",
        "\n",
        "  cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N,\n",
        "              d_C.cols, d_C.rows, d_A.cols, // M, N, K\n",
        "              &alpha,\n",
        "              d_B.data, d_B.cols, // B (K x N)\n",
        "              d_A.data, d_A.cols, // A (M x K)\n",
        "              &beta,\n",
        "              d_C.data, d_C.cols); // C (M x N)\n",
        "\n",
        "  cublasDestroy(handle);\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "/*MAIN SGEMM*/\n",
        "void gen_mat_mul(float alpha, fmatrix A, fmatrix B, float beta, fmatrix C, std::string arg){\n",
        "    if (arg == \"cpu\"){\n",
        "        matrixMultiplication_basic_host(alpha, A, B, beta, C);\n",
        "    } else {\n",
        "      if (arg == \"gpu_basic\"){\n",
        "          matrixMultiplication_basic(alpha, A, B, beta, C);\n",
        "\n",
        "      } else if (arg == \"gpu_tiled\"){\n",
        "          matrixMultiplication_tiled(alpha, A, B, beta, C);\n",
        "\n",
        "      } else if (arg == \"gpu_cublas\"){\n",
        "         matrixMultiplication_cublas(alpha, A, B, beta, C);\n",
        "\n",
        "      } else{\n",
        "          printf(\"Matrix Multiplication argument is Wrong\");\n",
        "          exit(0);\n",
        "      }\n",
        "      // wait for everything to finish\n",
        "      device_synchronize();\n",
        "    }\n",
        "}\n",
        "\n",
        "void mat_mul(fmatrix A, fmatrix B, fmatrix C, std::string arg){\n",
        " gen_mat_mul(1.0, A, B, 0.0, C, arg);\n",
        "}\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting sgemm.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnpSu2wH2ooy"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWEplkuA2Ygf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d475a8a3-1fdf-429d-c809-fc6607be38ca"
      },
      "source": [
        "%%writefile main.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include \"fmatrix.cuh\"\n",
        "#include \"sgemm.cuh\"\n",
        "\n",
        "#define TILE_WIDTH 32\n",
        "#define SIZE 40\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "  /* Allocate and initialize data on host */\n",
        "  fmatrix A = fmatrix_create_on_host(TILE_WIDTH * SIZE, TILE_WIDTH * SIZE);\n",
        "  fmatrix_init(A, 1.0);\n",
        "  fmatrix B = fmatrix_create_on_host(TILE_WIDTH * SIZE, TILE_WIDTH * SIZE);\n",
        "  fmatrix_init(B, 2.0);\n",
        "  fmatrix C = fmatrix_create_on_host(TILE_WIDTH * SIZE, TILE_WIDTH * SIZE);\n",
        "  fmatrix_init(C, 0.0);\n",
        "\n",
        "  /* Allocate data on device */\n",
        "  fmatrix d_A = fmatrix_create_on_device(TILE_WIDTH * SIZE, TILE_WIDTH * SIZE);\n",
        "  fmatrix d_B = fmatrix_create_on_device(TILE_WIDTH * SIZE, TILE_WIDTH * SIZE);\n",
        "  fmatrix d_C = fmatrix_create_on_device(TILE_WIDTH * SIZE, TILE_WIDTH * SIZE);\n",
        "\n",
        "  /* Transfer A and B on device */\n",
        "  fmatrix_data_to_device(A, d_A);\n",
        "  fmatrix_data_to_device(B, d_B);\n",
        "  fmatrix_data_to_device(C, d_C);\n",
        "\n",
        "  clock_t start, end;\n",
        "  float cpu_time_used;\n",
        "\n",
        "  /* Start calculation \"cpu\", \"gpu_basic\", \"gpu_tiled\", \"gpu_cublas\" */\n",
        "  /************** \"cpu\" *******************/\n",
        "  start = clock();\n",
        "  mat_mul(A, B, C, \"cpu\");\n",
        "  end = clock();\n",
        "  cpu_time_used = ((double) (end - start)) * 1000 / CLOCKS_PER_SEC;\n",
        "  printf(\"Time taken by CPU in milliseconds: %.2f\\n\", cpu_time_used);\n",
        "\n",
        "\n",
        "  /* Result correctness */\n",
        "  {\n",
        "    float maxError = 0.0f;\n",
        "    for (int i = 0; i < TILE_WIDTH * SIZE; i++){\n",
        "      for (int j = 0; j < TILE_WIDTH * SIZE; j++){\n",
        "        maxError = max(maxError, abs(getfm(C,i,j)- 2*TILE_WIDTH * SIZE));\n",
        "      }\n",
        "    }\n",
        "    printf(\"Max error: %f\\n\", maxError);\n",
        "  }\n",
        "  fmatrix_init(C, 0.0);\n",
        "\n",
        "  /************** \"gpu_basic\" *******************/\n",
        "  start = clock();\n",
        "  mat_mul(d_A, d_B, d_C, \"gpu_basic\");\n",
        "  end = clock();\n",
        "  cpu_time_used = ((double) (end - start)) * 1000 / CLOCKS_PER_SEC;\n",
        "  printf(\"GPU basic matrix multiplication in milliseconcs : %.2f\\n\", cpu_time_used);\n",
        "\n",
        "  /* Retrieve the result */\n",
        "  fmatrix_data_to_host(C, d_C);\n",
        "  /* Result correctness */\n",
        "  {\n",
        "    float maxError = 0.0f;\n",
        "    for (int i = 0; i < TILE_WIDTH * SIZE; i++){\n",
        "      for (int j = 0; j < TILE_WIDTH * SIZE; j++){\n",
        "        maxError = max(maxError, abs(getfm(C,i,j)- 2*TILE_WIDTH * SIZE));\n",
        "      }\n",
        "    }\n",
        "    printf(\"Max error: %f\\n\", maxError);\n",
        "  }\n",
        "  fmatrix_init(C, 0.0);\n",
        "  fmatrix_data_to_device(C, d_C);\n",
        "\n",
        "\n",
        " /************** \"gpu_tiled\" *******************/\n",
        "  start = clock();\n",
        "  mat_mul(d_A, d_B, d_C, \"gpu_tiled\");\n",
        "  end = clock();\n",
        "  cpu_time_used = ((double) (end - start)) * 1000 / CLOCKS_PER_SEC;\n",
        "  printf(\"GPU tiled matrix multiplication in milliseconcs : %.2f\\n\", cpu_time_used);\n",
        "\n",
        "  /* Retrieve the result */\n",
        "  fmatrix_data_to_host(C, d_C);\n",
        "  /* Result correctness */\n",
        "  {\n",
        "    float maxError = 0.0f;\n",
        "    for (int i = 0; i < TILE_WIDTH * SIZE; i++){\n",
        "      for (int j = 0; j < TILE_WIDTH * SIZE; j++){\n",
        "        maxError = max(maxError, abs(getfm(C,i,j)- 2*TILE_WIDTH * SIZE));\n",
        "      }\n",
        "    }\n",
        "    printf(\"Max error: %f\\n\", maxError);\n",
        "  }\n",
        "  fmatrix_init(C, 0.0);\n",
        "  fmatrix_data_to_device(C, d_C);\n",
        "\n",
        "\n",
        "  /************** \"gpu_cublas\" *******************/\n",
        "  for(int warmup = 0; warmup < 5; warmup++){\n",
        "    mat_mul(d_A, d_B, d_C, \"gpu_cublas\");\n",
        "  }\n",
        "  fmatrix_init(C, 0.0);\n",
        "  fmatrix_data_to_device(C, d_C);\n",
        "\n",
        "  start = clock();\n",
        "  mat_mul(d_A, d_B, d_C, \"gpu_cublas\");\n",
        "  end = clock();\n",
        "  cpu_time_used = ((double) (end - start)) * 1000 / CLOCKS_PER_SEC;\n",
        "  printf(\"GPU cuBLAS matrix multiplication in milliseconcs : %.2f\\n\", cpu_time_used);\n",
        "\n",
        "  /* Retrieve the result */\n",
        "  fmatrix_data_to_host(C, d_C);\n",
        "  /* Result correctness */\n",
        "  {\n",
        "    float maxError = 0.0f;\n",
        "    for (int i = 0; i < TILE_WIDTH * SIZE; i++){\n",
        "      for (int j = 0; j < TILE_WIDTH * SIZE; j++){\n",
        "        maxError = max(maxError, abs(getfm(C,i,j)- 2*TILE_WIDTH * SIZE));\n",
        "      }\n",
        "    }\n",
        "    printf(\"Max error: %f\\n\", maxError);\n",
        "  }\n",
        "  fmatrix_init(C, 0.0);\n",
        "  fmatrix_data_to_device(C, d_C);\n",
        "\n",
        "  /* Free */\n",
        "  fmatrix_free_on_host(&A);\n",
        "  fmatrix_free_on_host(&B);\n",
        "  fmatrix_free_on_host(&C);\n",
        "  fmatrix_free_on_device(&d_A);\n",
        "  fmatrix_free_on_device(&d_B);\n",
        "  fmatrix_free_on_device(&d_C);\n",
        "\n",
        "  cudaError_t err;\n",
        "  err = cudaPeekAtLastError();\n",
        "  if (err != cudaSuccess){\n",
        "      fprintf(stderr,\"GPUassert: add launch failed with the error : %s \\n\", cudaGetErrorString(err));\n",
        "      exit(err);\n",
        "  }\n",
        "  err = cudaDeviceSynchronize() ;\n",
        "  if (err != cudaSuccess){\n",
        "      fprintf(stderr,\"GPUassert: add execution failed with the error : %s \\n\", cudaGetErrorString(err));\n",
        "      exit(err);\n",
        "  }\n",
        "}"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting main.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrATC8s9LsDw"
      },
      "source": [
        "# Compiling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z52xd0NMRKXb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "700463d8-5084-4cdc-bb1c-934e473ed927"
      },
      "source": [
        "!nvcc -lcublas sgemm.cu  fmatrix.cu  cuda_stuff.cu main.cu -arch=sm_75 -o main"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[0m\u001b[01msgemm.cu(19)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"handle\"\u001b[0m was declared but never referenced\n",
            "  static cublasHandle_t handle;\n",
            "                        ^\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01msgemm.cu(20)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"cublas_init\"\u001b[0m was declared but never referenced\n",
            "  static int cublas_init = 0;\n",
            "             ^\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZVqTfXcLvPr"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_D8hNmXwi0S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2306cd4-14e2-4d5b-e2f3-b4f6633eb4db"
      },
      "source": [
        "! ./main"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken by CPU in milliseconds: 20439.76\n",
            "Max error: 0.000000\n",
            "GPU basic matrix multiplication in milliseconcs : 13.97\n",
            "Max error: 0.000000\n",
            "GPU tiled matrix multiplication in milliseconcs : 10.34\n",
            "Max error: 0.000000\n",
            "GPU cuBLAS matrix multiplication in milliseconcs : 1.95\n",
            "Max error: 0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color='LightBlue'> Analyse des résultats </font> #"
      ],
      "metadata": {
        "id": "FA29BjKCAiqW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il apparaît que pour des matrices de taille 40 * 32 = 1280, on obtient les résultats suivants :\n",
        "\n",
        "* Calcul classique sur CPU -> 18794 ms<br>\n",
        "* Calcul classique sur GPU -> 14.03 ms<br>\n",
        "* Calcul par tuile sur GPU -> 10.32 ms<br>\n",
        "* Calcul avec cuBLAS sur GPU -> 2.15 ms\n",
        "\n",
        "L'erreur nulle pour toutes les méthodes assure une justesse de calcul, ce qui les différencie sera donc leur temps d'exécution."
      ],
      "metadata": {
        "id": "oFNIw3osu_KU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour des matrices de taille 10, les temps de calcul obtenus sont les suivants :\n",
        "\n",
        "Calcul classique sur CPU : 0.03 ms<br>\n",
        "Calcul classique sur GPU : 0.19 ms<br>\n",
        "Calcul par tuiles sur GPU : 0.04 ms<br>\n",
        "Calcul avec cuBLAS sur GPU : 0.37 ms\n",
        "Ces résultats montrent qu'une taille de bloc fixe n'est pas toujours optimale, le calcul direct sur CPU étant le plus rapide dans ce cas.\n",
        "\n",
        "Nous avons également testé une taille de matrice plus grande, 1278, qui n'est pas un multiple de 32 mais reste proche de l'exemple initial. Les performances obtenues sont :\n",
        "\n",
        "Calcul classique sur CPU : 9999.47 ms<br>\n",
        "Calcul classique sur GPU : 14.23 ms<br>\n",
        "Calcul par tuiles sur GPU : 10.40 ms<br>\n",
        "Calcul avec cuBLAS sur GPU : 1.94 ms\n",
        "Bien que les calculs sur GPU semblent légèrement moins optimisés, les performances restent comparables. En revanche, sur CPU, la durée de calcul est presque divisée par deux lorsqu'on retire 2 lignes et 2 colonnes, ce qui marque une différence significative."
      ],
      "metadata": {
        "id": "BnNZ8tdL7A5O"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17aUKkNJqTDZ"
      },
      "source": [
        "# Debugging\n",
        "Compile with debugging info on the host (`-g`) and device (`-G`).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcfLGo1UrMq9"
      },
      "source": [
        "!nvcc -g -G -I /usr/local/cuda/samples/common/inc/ -L/usr/local/cuda/include -lcublas -lcusolver sgemm.cu fmatrix.cu cuda_stuff.cu main.cu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkuaGO10rRm9"
      },
      "source": [
        "Run the debugger cuda-gdb, stopping at the first error that is detected. Shows first the call stack on the GPU, the values of local variables, then the call stack on the host (thread 1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZ8nAtzGTRgH"
      },
      "source": [
        "! printf \"set cuda api_failures stop\\ncatch throw\\nr UNIT\\nbt\\ninfo locals\\nthread 1\\nbt\\n\" > tmp.txt\n",
        "! cat tmp.txt\n",
        "! cuda-gdb -batch -x tmp.txt ./a.out"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}