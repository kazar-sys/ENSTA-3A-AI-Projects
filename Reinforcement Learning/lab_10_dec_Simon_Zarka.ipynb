{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "# 4. Online control\n",
    "\n",
    "This notebook presents the **online control** of an agent by SARSA and Q-learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import TicTacToe, Nim, ConnectFour\n",
    "from agent import Agent, OnlineControl\n",
    "from dynamic import ValueIteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do\n",
    "\n",
    "* Complete the class ``SARSA`` and test it on Tic-Tac-Toe.\n",
    "* Complete the class ``QLearning`` and test it on Tic-Tac-Toe.\n",
    "* Compare these algorithms on Tic-Tac-Toe (play first) and Nim (play second), using a random adversary, then a perfect adversary. Comment your results.\n",
    "* Test these algorithms on Connect 4 against a random adversary. Comment your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSA(OnlineControl):\n",
    "    \"\"\"Online control by SARSA.\"\"\"\n",
    "        \n",
    "    def update_values(self, state=None, horizon=100, epsilon=0.5):\n",
    "        \"\"\"Learn the action-value function online.\"\"\"\n",
    "        self.model.reset(state)\n",
    "        state = self.model.state\n",
    "        if not self.model.is_terminal(state):\n",
    "            action = self.randomize_best_action(state, epsilon=epsilon)\n",
    "            for t in range(horizon):\n",
    "                self.add_state(state)\n",
    "                code = self.model.encode(state)\n",
    "                self.action_count[code][action] += 1\n",
    "                reward, stop = self.model.step(action)\n",
    "                # to be modified (get sample gain)\n",
    "                # begin\n",
    "                gain = reward\n",
    "                new_state = self.model.state\n",
    "                if not stop:\n",
    "                    new_action = self.randomize_best_action(new_state, epsilon=epsilon)\n",
    "                    gain = self.gamma * self.action_value[self.model.encode(new_state)][new_action]\n",
    "                # end\n",
    "                diff = gain - self.action_value[code][action]\n",
    "                count = self.action_count[code][action]\n",
    "                self.action_value[code][action] += diff / count\n",
    "                if stop:\n",
    "                    break\n",
    "                # to be modified (update state and action)\n",
    "                # begin\n",
    "                state = new_state\n",
    "                action = new_action\n",
    "                # end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearning(OnlineControl):\n",
    "    \"\"\"Online control by Q-learning.\"\"\"\n",
    "        \n",
    "    def update_values(self, state=None, horizon=100, epsilon=0.5):\n",
    "        \"\"\"Learn the action-value function online.\"\"\"\n",
    "        self.model.reset(state)\n",
    "        state = self.model.state\n",
    "        # to be completed\n",
    "        if not self.model.is_terminal(state):\n",
    "            action = self.randomize_best_action(state, epsilon=epsilon)\n",
    "            for t in range(horizon):\n",
    "                self.add_state(state)\n",
    "                code = self.model.encode(state)\n",
    "                self.action_count[code][action] += 1\n",
    "                reward, stop = self.model.step(action)\n",
    "                # to be modified (get sample gain)\n",
    "                # begin\n",
    "                gain = reward\n",
    "                new_state = self.model.state\n",
    "                if not stop:\n",
    "                    new_actions = self.get_best_actions(new_state)\n",
    "                    i = np.random.choice(len(new_actions))\n",
    "                    gain = self.gamma * self.action_value[self.model.encode(new_state)][new_actions[i]]\n",
    "                # end\n",
    "                diff = gain - self.action_value[code][action]\n",
    "                count = self.action_count[code][action]\n",
    "                self.action_value[code][action] += diff / count\n",
    "                if stop:\n",
    "                    break\n",
    "                # to be modified (update state and action)\n",
    "                # begin\n",
    "                state = new_state\n",
    "                action = new_actions[i]\n",
    "                # end\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TicTacToe against random player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1,  0,  1]), array([24, 16, 60], dtype=int64))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With random policy\n",
    "\n",
    "Game = TicTacToe\n",
    "game = Game()\n",
    "\n",
    "agent = Agent(game)\n",
    "\n",
    "np.unique(agent.get_gains(), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(agent.get_gains())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1,  1]), array([ 7, 93], dtype=int64))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With SARSA\n",
    "\n",
    "Control = SARSA\n",
    "algo = Control(game)\n",
    "\n",
    "n_games = 1000\n",
    "for i in range(n_games):\n",
    "    algo.update_values(epsilon=0.05)\n",
    "    \n",
    "policy = algo.get_policy()\n",
    "agent = Agent(game, policy)\n",
    "\n",
    "np.unique(agent.get_gains(), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(agent.get_gains())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1,  0,  1]), array([ 6,  4, 90], dtype=int64))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With QLearning\n",
    "\n",
    "Control = QLearning\n",
    "algo = Control(game)\n",
    "\n",
    "n_games = 1000\n",
    "for i in range(n_games):\n",
    "    algo.update_values(epsilon=0.05)\n",
    "    \n",
    "policy = algo.get_policy()\n",
    "agent = Agent(game, policy)\n",
    "\n",
    "np.unique(agent.get_gains(), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(agent.get_gains())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results for TicTacToe against a random player are the same as in SARSA and QLearning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nim against random player (play second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1,  1]), array([59, 41], dtype=int64))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With random policy\n",
    "\n",
    "Game = Nim\n",
    "game = Game(play_first=False)\n",
    "\n",
    "agent = Agent(game)\n",
    "\n",
    "np.unique(agent.get_gains(), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(agent.get_gains())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1,  1]), array([12, 88], dtype=int64))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With SARSA\n",
    "\n",
    "Control = SARSA\n",
    "algo = Control(game)\n",
    "\n",
    "n_games = 1000\n",
    "for i in range(n_games):\n",
    "    algo.update_values(epsilon=0.05)\n",
    "    \n",
    "policy = algo.get_policy()\n",
    "agent = Agent(game, policy)\n",
    "\n",
    "np.unique(agent.get_gains(), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.56"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(agent.get_gains())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1,  1]), array([25, 75], dtype=int64))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With QLearning\n",
    "\n",
    "Control = QLearning\n",
    "algo = Control(game)\n",
    "\n",
    "n_games = 1000\n",
    "for i in range(n_games):\n",
    "    algo.update_values(epsilon=0.05)\n",
    "    \n",
    "policy = algo.get_policy()\n",
    "agent = Agent(game, policy)\n",
    "\n",
    "np.unique(agent.get_gains(), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(agent.get_gains())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results as a Nim second player against a random adversary are the same between SARSA and QLearning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TicTacToe agains a perfect player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = ValueIteration(TicTacToe(), gamma=0.9)\n",
    "policy, adversary_policy = algo.get_perfect_players()\n",
    "\n",
    "Game = TicTacToe\n",
    "game = Game(adversary_policy=adversary_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1,  0]), array([50, 50], dtype=int64))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With SARSA\n",
    "\n",
    "Control = SARSA\n",
    "algo = Control(game)\n",
    "\n",
    "n_games = 1000\n",
    "for i in range(n_games):\n",
    "    algo.update_values(epsilon=0.05)\n",
    "    \n",
    "policy = algo.get_policy()\n",
    "agent = Agent(game, policy)\n",
    "\n",
    "np.unique(agent.get_gains(), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.49"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(agent.get_gains())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1,  0]), array([39, 61], dtype=int64))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With QLearning\n",
    "\n",
    "Control = QLearning\n",
    "algo = Control(game)\n",
    "\n",
    "n_games = 1000\n",
    "for i in range(n_games):\n",
    "    algo.update_values(epsilon=0.05)\n",
    "    \n",
    "policy = algo.get_policy()\n",
    "agent = Agent(game, policy)\n",
    "\n",
    "np.unique(agent.get_gains(), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.42"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(agent.get_gains())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SARSA and QLearning policies can't win over perfect players."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nim against perfect player (play second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = ValueIteration(Nim(play_first=False), gamma=0.9)\n",
    "policy, adversary_policy = algo.get_perfect_players()\n",
    "\n",
    "Game = Nim\n",
    "game = Game(adversary_policy=adversary_policy, play_first=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1,  1]), array([59, 41], dtype=int64))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With SARSA\n",
    "\n",
    "Control = SARSA\n",
    "algo = Control(game)\n",
    "\n",
    "n_games = 1000\n",
    "for i in range(n_games):\n",
    "    algo.update_values(epsilon=0.05)\n",
    "    \n",
    "policy = algo.get_policy()\n",
    "agent = Agent(game, policy)\n",
    "\n",
    "np.unique(agent.get_gains(), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(agent.get_gains())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1,  1]), array([ 4, 96], dtype=int64))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With QLearning\n",
    "\n",
    "Control = QLearning\n",
    "algo = Control(game)\n",
    "\n",
    "n_games = 1000\n",
    "for i in range(n_games):\n",
    "    algo.update_values(epsilon=0.05)\n",
    "    \n",
    "policy = algo.get_policy()\n",
    "agent = Agent(game, policy)\n",
    "\n",
    "np.unique(agent.get_gains(), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(agent.get_gains())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Against a perfect player in a Nim game, if playing second, SARSA policy can make you win.\n",
    "\n",
    "However, QLearning policy always win against a perfect player, because by playing first, the perfect player knows it can't win, and that QLearning learns moves powerful enough to make us win all the time, Nim being a game with practically no chance involved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConnectFour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1,  1]), array([46, 54], dtype=int64))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With random policy\n",
    "\n",
    "Game = ConnectFour\n",
    "game = Game()\n",
    "\n",
    "agent = Agent(game)\n",
    "\n",
    "np.unique(agent.get_gains(), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(agent.get_gains())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1,  1]), array([41, 59], dtype=int64))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With SARSA\n",
    "\n",
    "Control = SARSA\n",
    "algo = Control(game)\n",
    "\n",
    "n_games = 1000\n",
    "for i in range(n_games):\n",
    "    algo.update_values(epsilon=0.05)\n",
    "    \n",
    "policy = algo.get_policy()\n",
    "agent = Agent(game, policy)\n",
    "\n",
    "np.unique(agent.get_gains(), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(agent.get_gains())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1,  1]), array([37, 63], dtype=int64))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With QLearning\n",
    "\n",
    "Control = QLearning\n",
    "algo = Control(game)\n",
    "\n",
    "n_games = 1000\n",
    "for i in range(n_games):\n",
    "    algo.update_values(epsilon=0.05)\n",
    "    \n",
    "policy = algo.get_policy()\n",
    "agent = Agent(game, policy)\n",
    "\n",
    "np.unique(agent.get_gains(), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(agent.get_gains())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SARSA and QLearning based policies give better results than random policy against a random player.\n",
    "\n",
    "These algorithms allow us to play ConnectFour, as they don't need to memorize all states, contrary to Dynamic Programming. Nevertheless the computation is very long."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
